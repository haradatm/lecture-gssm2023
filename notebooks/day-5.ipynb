{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6f6ed6-79e9-4efc-836a-040000ce8c89",
   "metadata": {
    "tags": []
   },
   "source": [
    "# day 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37b43ff",
   "metadata": {},
   "source": [
    "このノートブックの実行例は[こちら(HTML版)](../notebooks-sample/day-5.html)で確認できます"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7303c7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d9bc8c-8096-4d76-b983-250b7a4b8203",
   "metadata": {},
   "source": [
    "## 0. はじめに"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a794a09-51b9-449b-b699-32a6318de81b",
   "metadata": {},
   "source": [
    "ページ上部のメニューバーにある **Kernel** メニューをクリックし、プルダウンメニューから [**Change Kernel ...**] を選び、**gssm2023:Python** を選択してください。\n",
    "\n",
    "<img src=\"images/change_kernel1.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097064cc-d520-4893-a0e6-16ff5b34c15a",
   "metadata": {},
   "source": [
    "ノートブック上部の右隅に表示されたカーネル名が **gssm2023:Python** になっていることを確認してください。\n",
    "\n",
    "<img src=\"images/change_kernel2.png\" width=\"30%\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038e0af-8843-493e-831f-2bceee5b6656",
   "metadata": {},
   "source": [
    "## 1. テキスト分析 (実践編)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e3e63",
   "metadata": {},
   "source": [
    "### 1.0 事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad7a6b",
   "metadata": {},
   "source": [
    "#### 1.0.1 関数の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb3e5b",
   "metadata": {},
   "source": [
    "以下のセルを**修正せず**に実行してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b83452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 乱数を固定する\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# ワードクラウドを描画する\n",
    "def plot_wordcloud(word_str, width=6, height=4):\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # ノートブック中にプロットするマジックコマンド\n",
    "    %matplotlib inline\n",
    "\n",
    "    # プロットの準備\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    # 指定したプロット位置(ax)にワードクラウドを描画する\n",
    "    plot_wordcloud_ax(ax, word_str)\n",
    "\n",
    "    # プロットの仕上げ\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 指定したプロット位置(ax)にワードクラウドを描画する\n",
    "def plot_wordcloud_ax(ax, word_str):\n",
    "\n",
    "    # フォントパスを取得する\n",
    "    font_path = !find ${HOME} -name \"ipaexg.ttf\"\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import wordcloud\n",
    "\n",
    "    # ワードクラウドを作成する\n",
    "    wc = wordcloud.WordCloud(\n",
    "        background_color='white',\n",
    "        font_path=font_path[0],\n",
    "        max_font_size=100)\n",
    "\n",
    "    # ワードクラウドを描画する\n",
    "    img = wc.generate(word_str)\n",
    "    ax.imshow(img, interpolation='bilinear')\n",
    "\n",
    "\n",
    "# トピックモデルによるワードクラウドを描画する\n",
    "def plot_topic_model(lda, feature_names, n_top_words=20, width=10, height=4):\n",
    "\n",
    "    # フォントパスを取得する\n",
    "    font_path = !find ${HOME} -name \"ipaexg.ttf\"\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import matplotlib.pyplot as plt\n",
    "    import wordcloud\n",
    "\n",
    "    # ノートブック中にプロットするマジックコマンド\n",
    "    %matplotlib inline\n",
    "\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "\n",
    "    # トピックごとのループ\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "\n",
    "        # トピック中で出現確率の高い頻に単語をソートし,\n",
    "        # ワードクラウドに描画するためテキストを生成する\n",
    "        sorted_text = ' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]])\n",
    "\n",
    "        # ワードクラウドを作成する\n",
    "        wc = wordcloud.WordCloud(\n",
    "            background_color='white',\n",
    "            font_path=font_path[0],\n",
    "            max_font_size=100)\n",
    "\n",
    "        # プロット位置(ax)を選ぶ\n",
    "        ax = fig.add_subplot(2, 3, topic_idx + 1)\n",
    "\n",
    "        # ワードクラウドを描画する\n",
    "        img = wc.generate(sorted_text)\n",
    "        ax.imshow(img, interpolation='bilinear')\n",
    "        ax.set_title(f\"Topic # {topic_idx+1}:\")\n",
    "\n",
    "    # プロットの仕上げ\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 共起ネットワーク図を描画する (抽出語-抽出語用)\n",
    "def plot_cooccur_network(df, word_counts, cutoff, width=8, height=8):\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import matplotlib.pyplot as plt\n",
    "    import japanize_matplotlib\n",
    "\n",
    "    # プロットの準備\n",
    "    plt.figure(figsize=(width, height))\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "\n",
    "    # プロット位置(ax)を選ぶ\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    # 指定したプロット位置(ax)に共起ネットワーク図を描画する\n",
    "    plot_cooccur_network_ax(ax, df, word_counts, cutoff)\n",
    "\n",
    "    # プロットの仕上げ\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# 指定したプロット位置(ax)に共起ネットワーク図を描画する\n",
    "def plot_cooccur_network_ax(ax, df, word_counts, cutoff):\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    from networkx.algorithms import community\n",
    "    from networkx.drawing.nx_agraph import graphviz_layout\n",
    "    import matplotlib.pyplot as plt\n",
    "    import japanize_matplotlib\n",
    "\n",
    "    # ノートブック中にプロットするマジックコマンド\n",
    "    %matplotlib inline\n",
    "\n",
    "    # 共起行列の中身(numpy行列)を取り出す\n",
    "    Xc = df.values\n",
    "\n",
    "    # 共起行列中の最大値を求める\n",
    "    Xc_max = Xc.max()\n",
    "\n",
    "    # プロットする単語リストを取得する\n",
    "    words = df.columns\n",
    "\n",
    "    # プロットする単語の出現頻度の最大値を求める (正規化用)\n",
    "    count_max = word_counts.max()\n",
    "\n",
    "    weights_w, weights_c = [], []\n",
    "\n",
    "    # 共起行列の要素ごとのループ (値がゼロの要素はスキップ)\n",
    "    for i, j in zip(*Xc.nonzero()):\n",
    "        # 対角行列でかつ値がしきい値を超えるものを保持する\n",
    "        if i < j and Xc[i,j] > cutoff:\n",
    "            # ノード: 一方の単語とノードの大きさ(正規化した出現頻度)を保持する\n",
    "            weights_w.append((words[i], {'weight': word_counts[i] / count_max}))\n",
    "            # ノード: 他方の単語とノードの大きさ(正規化した出現頻度)を保持する\n",
    "            weights_w.append((words[j], {'weight': word_counts[j] / count_max}))\n",
    "            # エッジ: 両端の単語を結ぶエッジの太さ(正規化した共起行列の値)を保持する\n",
    "            weights_c.append((words[i], words[j], Xc[i,j] / Xc_max))\n",
    "\n",
    "    # グラフの作成\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(weights_w)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    # G = nx.minimum_spanning_tree(G)\n",
    "    # pos = nx.spring_layout(G, k=0.3)\n",
    "    pos = graphviz_layout(G, prog='neato', args='-Goverlap=\"scalexy\" -Gsep=\"+6\" -Gnodesep=0.8 -Gsplines=\"polyline\" -GpackMode=\"graph\" -Gstart={}'.format(43))\n",
    "\n",
    "    # プロット用にノートとエッジの重みをリストに変換する\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, 'weight').values()))\n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
    "\n",
    "    # サブグラフの検出\n",
    "    communities = community.greedy_modularity_communities(G)\n",
    "    color_map = []\n",
    "    for node in G:\n",
    "        for i, c in enumerate(communities):\n",
    "            if node in c:\n",
    "                color_map.append(i)\n",
    "\n",
    "    # グラフの描画\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=color_map, alpha=0.7, cmap=plt.cm.Set2, node_size=5000 * weights_n, ax=ax)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', edge_cmap=plt.cm.Blues, alpha=0.7, width=3 * weights_e, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', ax=ax)\n",
    "    # ax.axis('off')\n",
    "\n",
    "\n",
    "# 指定したプロット位置(ax)に共起ネットワーク図を描画する\n",
    "def plot_cooccur_network_with_code_ax(ax, df, word_counts, cutoff, coding_rule=None):\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    from networkx.algorithms import community\n",
    "    from networkx.drawing.nx_agraph import graphviz_layout\n",
    "    import matplotlib.pyplot as plt\n",
    "    import japanize_matplotlib\n",
    "\n",
    "    # ノートブック中にプロットするマジックコマンド\n",
    "    %matplotlib inline\n",
    "\n",
    "    # 共起行列の中身(numpy行列)を取り出す\n",
    "    Xc = df.values\n",
    "\n",
    "    # 共起行列中の最大値を求める\n",
    "    Xc_max = Xc.max()\n",
    "\n",
    "    # プロットする単語リストを取得する\n",
    "    words = df.columns\n",
    "\n",
    "    # プロットする単語の出現頻度の最大値を求める (正規化用)\n",
    "    count_max = word_counts.max()\n",
    "\n",
    "    weights_w, weights_c = [], []\n",
    "\n",
    "    # 共起行列の要素ごとのループ (値がゼロの要素はスキップ)\n",
    "    for i, j in zip(*Xc.nonzero()):\n",
    "        # 対角行列でかつ値がしきい値を超えるものを保持する\n",
    "        if i < j and Xc[i,j] > cutoff:\n",
    "            # ノード: 一方の単語とノードの大きさ(正規化した出現頻度)を保持する\n",
    "            weights_w.append((words[i], {'weight': word_counts[i] / count_max}))\n",
    "            # ノード: 他方の単語とノードの大きさ(正規化した出現頻度)を保持する\n",
    "            weights_w.append((words[j], {'weight': word_counts[j] / count_max}))\n",
    "            # エッジ: 両端の単語を結ぶエッジの太さ(正規化した共起行列の値)を保持する\n",
    "            weights_c.append((words[i], words[j], Xc[i,j] / Xc_max))\n",
    "\n",
    "    # グラフの作成\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(weights_w)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    # G = nx.minimum_spanning_tree(G)\n",
    "    # pos = nx.spring_layout(G, k=0.3)\n",
    "    pos = graphviz_layout(G, prog='neato', args='-Goverlap=\"scalexy\" -Gsep=\"+6\" -Gnodesep=0.8 -Gsplines=\"polyline\" -GpackMode=\"graph\" -Gstart={}'.format(43))\n",
    "\n",
    "    # プロット用にノートとエッジの重みをリストに変換する\n",
    "    nodelist_c = [node for node in G.nodes if node in coding_rule]\n",
    "    nodelist_w = [node for node in G.nodes if node not in coding_rule]\n",
    "    weights_c = np.array([G.nodes[node]['weight'] for node in G.nodes if node in coding_rule])\n",
    "    weights_w = np.array([G.nodes[node]['weight'] for node in G.nodes if node not in coding_rule])\n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
    "\n",
    "    # サブグラフの検出\n",
    "    communities = community.greedy_modularity_communities(G)\n",
    "    color_map_c = []\n",
    "    for node in G:\n",
    "        if node in coding_rule:\n",
    "            for i, c in enumerate(communities):\n",
    "                if node in c:\n",
    "                    color_map_c.append(i)\n",
    "    color_map_w = []\n",
    "    for node in G:\n",
    "        if node not in coding_rule:\n",
    "            for i, c in enumerate(communities):\n",
    "                if node in c:\n",
    "                    color_map_w.append(i)\n",
    "\n",
    "    # グラフの描画\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=color_map_c, alpha=0.7, cmap=plt.cm.Set2, node_size=5000 * weights_c, ax=ax, nodelist=nodelist_c, node_shape='s', edgecolors='red')\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=color_map_w, alpha=0.7, cmap=plt.cm.Set2, node_size=5000 * weights_w, ax=ax, nodelist=nodelist_w)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', edge_cmap=plt.cm.Blues, alpha=0.7, width=3 * weights_e, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', ax=ax)\n",
    "    # ax.axis('off')\n",
    "\n",
    "\n",
    "# 共起ネットワークを描画する (外部変数-抽出語用)\n",
    "def plot_attrs_network(df, attr_counts, word_counts, cutoff, width=8, height=8):\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "    import japanize_matplotlib\n",
    "    from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "    # ノートブック中にプロットするマジックコマンド\n",
    "    %matplotlib inline\n",
    "\n",
    "    # 共起行列の中身(numpy行列)を取り出す\n",
    "    Xc = df.values\n",
    "\n",
    "    # 共起行列中の最大値を求める\n",
    "    Xc_max = Xc.max()\n",
    "\n",
    "    # プロットする属性(外部変数等)リストを取得する\n",
    "    attrs = list(df.index)\n",
    "\n",
    "    # プロットする属性(外部変数等)の最大数を求める (正規化用)\n",
    "    attr_count_max = attr_counts.max()\n",
    "\n",
    "    # プロットする単語リストを取得する\n",
    "    words = list(df.columns)\n",
    "\n",
    "    # プロットする単語の出現頻度の最大値を求める (正規化用)\n",
    "    word_count_max = word_counts.max()\n",
    "\n",
    "    weights_n, weights_c = [], []\n",
    "\n",
    "    # 共起行列の要素ごとのループ\n",
    "    for i, j in zip(*Xc.nonzero()):\n",
    "        # 値がしきい値を超えるものを保持する (値がゼロの要素はスキップ)\n",
    "        if Xc[i,j] > cutoff:\n",
    "            # ノード: 属性(外部変数等)とノードの大きさ(正規化した属性数)を保持する\n",
    "            weights_n.append((attrs[i], {'weight': attr_counts[i] / attr_count_max, 'type': 'attr'}))\n",
    "            # ノード: 単語とノードの大きさ(正規化した出現頻度)を保持する\n",
    "            weights_n.append((words[j], {'weight': word_counts[j] / word_count_max, 'type': 'word'}))\n",
    "            # エッジ: 属性(外部変数等)と単語を結ぶエッジの太さ(正規化した共起行列の値)を保持する\n",
    "            weights_c.append((attrs[i], words[j], Xc[i,j] / Xc_max))\n",
    "\n",
    "    # プロットの準備\n",
    "    plt.figure(figsize=(width, height))\n",
    "\n",
    "    # グラフの作成\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(weights_n)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    # G = nx.minimum_spanning_tree(G)\n",
    "    # pos = nx.spring_layout(G, k=0.3)\n",
    "    pos = graphviz_layout(G, prog='neato', args='-Goverlap=\"scalexy\" -Gsep=\"+6\" -Gnodesep=0.8 -Gsplines=\"polyline\" -GpackMode=\"graph\" -Gstart={}'.format(43))\n",
    "\n",
    "    # プロット用にノートとエッジの重みをリストに変換する\n",
    "    nodelist_a = [node for node in G.nodes if G.nodes[node]['type'] == 'attr']\n",
    "    nodelist_w = [node for node in G.nodes if G.nodes[node]['type'] == 'word']\n",
    "    weights_a = np.array([G.nodes[node]['weight'] for node in G.nodes if G.nodes[node]['type'] == 'attr'])\n",
    "    weights_w = np.array([G.nodes[node]['weight'] for node in G.nodes if G.nodes[node]['type'] == 'word'])\n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
    "\n",
    "    # 属性と単語を色分けする\n",
    "    color_map = []\n",
    "    for node in G:\n",
    "        if G.nodes[node]['type'] == 'word':\n",
    "            color_map.append(G.degree(node)+3)   # \"+3\"はカラーマップをシフトする調整値\n",
    "\n",
    "    # グラフの描画\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightsalmon', alpha=0.7, cmap=plt.cm.Set2, node_size=1000 * weights_a, nodelist=nodelist_a, node_shape='s')\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=color_map, alpha=0.7, cmap=plt.cm.Set2, node_size=5000 * weights_w, nodelist=nodelist_w)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', edge_cmap=plt.cm.Blues, alpha=0.7, width=3 * weights_e)\n",
    "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic')\n",
    "\n",
    "    # プロットの仕上げ\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 係り受けによる共起ネットワークを描画する\n",
    "def plot_dependency_network(df, word_counts, cutoff, width=8, height=8):\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    from networkx.algorithms import community\n",
    "    import matplotlib.pyplot as plt\n",
    "    import japanize_matplotlib\n",
    "    from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "    # ノートブック中にプロットするマジックコマンド\n",
    "    %matplotlib inline\n",
    "\n",
    "    # 共起行列の中身(numpy行列)を取り出す\n",
    "    Xc = df.values\n",
    "\n",
    "    # 共起行列中の最大値を求める\n",
    "    Xc_max = Xc.max()\n",
    "\n",
    "    # プロットする単語リストを取得する\n",
    "    words = df.columns\n",
    "\n",
    "    # プロットする単語の出現頻度の最大値を求める (正規化用)\n",
    "    count_max = word_counts.max()\n",
    "\n",
    "    weights_w, weights_c = [], []\n",
    "\n",
    "    # 共起行列の要素ごとのループ\n",
    "    for i, j in zip(*Xc.nonzero()):\n",
    "        # 対角行列でかつ値がしきい値を超えるものを保持する (値がゼロの要素はスキップ)\n",
    "        if i != j and Xc[i,j] > cutoff:\n",
    "            # ノード: 一方の単語とノードの大きさ(正規化した出現頻度)を保持する\n",
    "            weights_w.append((words[i], {'weight': word_counts[i] / count_max}))\n",
    "            # ノード: 他方の単語とノードの大きさ(正規化した出現頻度)を保持する\n",
    "            weights_w.append((words[j], {'weight': word_counts[j] / count_max}))\n",
    "            # エッジ: 両端の単語を結ぶエッジの太さ(正規化した共起行列の値)を保持する\n",
    "            weights_c.append((words[i], words[j], Xc[i,j] / Xc_max))\n",
    "\n",
    "    # プロットの準備\n",
    "    plt.figure(figsize=(width, height))\n",
    "\n",
    "    # グラフの作成\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(weights_w)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    # G = nx.minimum_spanning_tree(G)\n",
    "    # pos = nx.spring_layout(G, k=0.3)\n",
    "    pos = graphviz_layout(G, prog='neato', args='-Goverlap=\"scalexy\" -Gsep=\"+6\" -Gnodesep=0.8 -Gsplines=\"polyline\" -GpackMode=\"graph\" -Gstart={}'.format(43))\n",
    "\n",
    "    # プロット用にノートとエッジの重みをリストに変換する\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, 'weight').values()))\n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
    "\n",
    "    # サブグラフの検出\n",
    "    communities = community.greedy_modularity_communities(G)\n",
    "    color_map = []\n",
    "    for node in G:\n",
    "        for i, c in enumerate(communities):\n",
    "            if node in c:\n",
    "                color_map.append(i)\n",
    "\n",
    "    # グラフの描画\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=color_map, alpha=0.7, cmap=plt.cm.Set2, node_size=5000 * weights_n)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', edge_cmap=plt.cm.Blues, alpha=0.7, width=3 * weights_e)\n",
    "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic')\n",
    "\n",
    "    # プロットの仕上げ\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 対応分析の結果をプロットする\n",
    "def plot_coresp(row_coord, col_coord, row_labels, col_labels, explained_inertia=None, width=8, height=8):\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import matplotlib.pyplot as plt\n",
    "    import japanize_matplotlib\n",
    "\n",
    "    # ノートブック中にプロットするマジックコマンド\n",
    "    %matplotlib inline\n",
    "\n",
    "    # プロットの準備\n",
    "    # plt.figure(figsize=(width, height))\n",
    "\n",
    "    # 行方向(外部変数)のプロット\n",
    "    plt.plot(row_coord[:, 0], row_coord[:, 1], \"*\", color='red', alpha=0.5)\n",
    "    for i, label in enumerate(row_labels):\n",
    "        plt.text(row_coord[i, 0], row_coord[i, 1], label, color='red', ha='left', va='bottom')\n",
    "\n",
    "    # 列方向(単語)のプロット\n",
    "    plt.plot(col_coord[:, 0], col_coord[:, 1], \"o\", color='blue', alpha=0.5)\n",
    "    for i, label in enumerate(col_labels):\n",
    "        plt.text(col_coord[i, 0], col_coord[i, 1], label, color='blue', ha='left', va='bottom')\n",
    "\n",
    "    # 原点を通る水平と垂直線を引く\n",
    "    plt.axvline(0, linestyle='dashed', color='gray', alpha=0.5)\n",
    "    plt.axhline(0, linestyle='dashed', color='gray', alpha=0.5)\n",
    "\n",
    "    # 軸ラベルに寄与率を追記する\n",
    "    if explained_inertia is not None:\n",
    "        plt.xlabel(f\"Dim 1 ({explained_inertia[0]:.3f}%)\")\n",
    "        plt.ylabel(f\"Dim 2 ({explained_inertia[1]:.3f}%)\")\n",
    "\n",
    "    # プロットの仕上げ\n",
    "    # plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# PCA の結果をプロットする\n",
    "def plot_pca(coeff, reduced, row_labels, col_labels, var_ratio=None, width=8, height=8):\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import matplotlib.pyplot as plt\n",
    "    import japanize_matplotlib\n",
    "\n",
    "    # ノートブック中にプロットするマジックコマンド\n",
    "    %matplotlib inline\n",
    "\n",
    "    # プロットの準備\n",
    "    # plt.figure(figsize=(width, height))\n",
    "\n",
    "    # 行方向(外部変数)のプロット\n",
    "    for i, label in enumerate(row_labels):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color='r', alpha=0.5)\n",
    "        plt.text(coeff[i, 0], coeff[i, 1], label, color='red', ha='left', va='bottom')\n",
    "\n",
    "    # 列方向(単語)のプロット\n",
    "    plt.plot(reduced[:, 0], reduced[:, 1], \"o\", color='blue', alpha=0.5)\n",
    "    for i, label in enumerate(col_labels):\n",
    "        plt.text(reduced[i, 0], reduced[i, 1], label, color='blue', ha='left', va='bottom')\n",
    "\n",
    "    # 原点を通る水平と垂直線を引く\n",
    "    plt.axvline(0, linestyle='dashed', color='gray', alpha=0.5)\n",
    "    plt.axhline(0, linestyle='dashed', color='gray', alpha=0.5)\n",
    "\n",
    "    # 軸ラベルに寄与率を追記する\n",
    "    if var_ratio is not None:\n",
    "        plt.xlabel(f\"Dim 1 ({var_ratio[0]*100:.3f}%)\")\n",
    "        plt.ylabel(f\"Dim 2 ({var_ratio[1]*100:.3f}%)\")\n",
    "\n",
    "    # プロットの仕上げ\n",
    "    # plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 共起頻度行列を Jaccard 係数行列に変換する (抽出語-抽出語用)\n",
    "def jaccard_coef(cooccur_df, cross_df):\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # 共起行列の中身(numpy行列)を取り出す\n",
    "    Xc = cooccur_df.values\n",
    "\n",
    "    # Jaccard 係数行列を初期化する (共起行列と同じ形)\n",
    "    Xj = np.zeros(Xc.shape)\n",
    "\n",
    "    # 単語ごとに共起度を集計する\n",
    "    Xc_sum = cross_df.sum(axis=0).values\n",
    "\n",
    "    # 共起行列の要素ごとのループ (値がゼロの要素はスキップ)\n",
    "    for i, j in zip(*Xc.nonzero()):\n",
    "        # 対角行列の要素を取り出す\n",
    "        if i < j:\n",
    "            # Jaccard 係数を求める\n",
    "            Xj[i,j] = Xc[i,j] / (Xc_sum[i] + Xc_sum[j] - Xc[i,j])\n",
    "\n",
    "    # DataFrame 型に整える\n",
    "    jaccard_df = pd.DataFrame(Xj, columns=cooccur_df.columns, index=cooccur_df.columns)\n",
    "\n",
    "    return jaccard_df\n",
    "\n",
    "\n",
    "# 共起頻度行列を Jaccard 係数行列に変換する (外部変数-抽出語用)\n",
    "def jaccard_attrs_coef(df, attr_counts, word_counts, total=10000, conditional=False):\n",
    "\n",
    "    # 必要ライブラリのインポート\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # 共起行列の中身(numpy行列)を取り出す\n",
    "    Xc = df.values\n",
    "\n",
    "    # Jaccard 係数行列を初期化する (共起行列と同じ形)\n",
    "    Xj = np.zeros(df.shape)\n",
    "\n",
    "    # 共起行列の要素ごとのループ (値がゼロの要素はスキップ)\n",
    "    for i, j in zip(*Xc.nonzero()):\n",
    "\n",
    "        # conditional フラグが True の場合, 条件付き確率 > 前提確率 以外はゼロにする\n",
    "        if not conditional:\n",
    "\n",
    "            # 条件付き確率を求める\n",
    "            conditional_prob = Xc[i,j] / attr_counts[i]\n",
    "\n",
    "            # 前提確率を求める\n",
    "            assumption_prob = word_counts[j] / total\n",
    "\n",
    "            # 条件付き確率 > 前提確率の場合\n",
    "            if conditional_prob > assumption_prob:\n",
    "                # Jaccard 係数を求める\n",
    "                Xj[i,j] = Xc[i,j] / (attr_counts[i] + word_counts[j] - Xc[i,j])\n",
    "\n",
    "            # 条件付き確率 <= 前提確率の場合\n",
    "            else:\n",
    "                # ゼロにする\n",
    "                Xj[i,j] = .0\n",
    "\n",
    "        # conditional フラグが False の場合, すべてのケースで Jaccard 係数を求める (デフォルト)\n",
    "        else:\n",
    "            # Jaccard 係数を求める\n",
    "            Xj[i,j] = Xc[i,j] / (attr_counts[i] + word_counts[j] - Xc[i,j])\n",
    "\n",
    "    # DataFrame 型に整える\n",
    "    jaccard_df = pd.DataFrame(Xj, columns=df.columns, index=df.index)\n",
    "\n",
    "    return jaccard_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867f595",
   "metadata": {},
   "source": [
    "#### 1.0.1 データのダウンロード (前回ダウンロード済みのためスキップ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368d5e8",
   "metadata": {},
   "source": [
    "以下のデータがダウンロード済みです"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27502dbc",
   "metadata": {},
   "source": [
    "| ファイル名 | 件数 | データセット | 備考 |\n",
    "| --- | --- | --- | --- |\n",
    "| rakuten-1000-2022-2023.xlsx.zip | 10,000 | •レジャー+ビジネスの 10エリア<br>•エリアごと 1,000件 (ランダムサンプリング)<br>•期間: 2022/1~2023 GW明け | 本講義の全体を通して使用する |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# もし、再度ダウンロードが必要な場合は残りの行のコメントマーク「#」を除去して、このセルを再実行してください\n",
    "\n",
    "# FILE_ID = \"1n-uvGoH7XQhxexN57hYXuFrkGeHKp-HV\"\n",
    "# !gdown --id {FILE_ID}\n",
    "# !unzip rakuten-1000-2022-2023.xlsx.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba6d12",
   "metadata": {},
   "source": [
    "#### 1.0.2 データの読み込み (DataFrame型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "all_df = pd.read_excel(\"rakuten-1000-2022-2023.xlsx\")\n",
    "print(all_df.shape)\n",
    "display(all_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c983a2",
   "metadata": {},
   "source": [
    "#### 1.0.3 「文書-抽出語」表 を作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99c1ac",
   "metadata": {},
   "source": [
    "コメント列から単語を抽出する (単語を品詞「名詞」「形容詞」「未知語」で絞り込む)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f750bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "from collections import defaultdict\n",
    "import MeCab\n",
    "\n",
    "# mecab の初期化\n",
    "tagger = MeCab.Tagger(\"-r ../tools/usr/local/etc/mecabrc --unk-feature 未知語\")\n",
    "\n",
    "# 単語頻度辞書の初期化\n",
    "word_counts = defaultdict(lambda: 0)\n",
    "\n",
    "# 抽出語情報リストの初期化\n",
    "words = []\n",
    "\n",
    "# 半角->全角変換マクロを定義する\n",
    "ZEN = \"\".join(chr(0xff01 + i) for i in range(94))\n",
    "HAN = \"\".join(chr(0x21 + i) for i in range(94))\n",
    "HAN2ZEN = str.maketrans(HAN, ZEN)\n",
    "\n",
    "# ストップワードを定義する\n",
    "# stopwords = ['する', 'ある', 'ない', 'いう', 'もの', 'こと', 'よう', 'なる', 'ほう']\n",
    "stopwords = [\"湯畑\"]\n",
    "\n",
    "# データ1行ごとのループ\n",
    "for index, row in all_df.iterrows():\n",
    "\n",
    "    # 半角->全角変換した後で, mecab で解析する\n",
    "    node = tagger.parseToNode(row[\"コメント\"].translate(HAN2ZEN))\n",
    "\n",
    "    # 形態素ごとのループ\n",
    "    while node:\n",
    "        # 解析結果を要素ごとにバラす\n",
    "        features = node.feature.split(',')\n",
    "\n",
    "        # 品詞1 を取り出す\n",
    "        pos1 = features[0]\n",
    "\n",
    "        # 品詞2 を取り出す\n",
    "        pos2 = features[1] if len(features) > 1 else \"\"\n",
    "\n",
    "        # 原形 を取り出す\n",
    "        base = features[6] if len(features) > 6 else None\n",
    "\n",
    "        # 原型がストップワードに含まれない単語のみ抽出する\n",
    "        if base not in stopwords:\n",
    "\n",
    "            # 「名詞-一般」\n",
    "            if (pos1 == \"名詞\" and pos2 == \"一般\"):\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"名詞\"\n",
    "                key = (base, postag)\n",
    "\n",
    "                # 単語頻度辞書をカウントアップする\n",
    "                word_counts[key] += 1\n",
    "\n",
    "                # 抽出語情報をリストに追加する\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "            # 「形容動詞」\n",
    "            elif (pos1 == \"名詞\" and pos2 == \"形容動詞語幹\"):\n",
    "                base = base if base is not None else node.surface\n",
    "                base = f\"{base}\"\n",
    "                postag = \"形容動詞\"\n",
    "                key = (base, postag)\n",
    "\n",
    "                # 単語頻度辞書をカウントアップする\n",
    "                word_counts[key] += 1\n",
    "\n",
    "                # 抽出語情報をリストに追加する\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "            # 「形容詞」\n",
    "            elif pos1 == \"形容詞\":\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"形容詞\"\n",
    "                key = (base, postag)\n",
    "\n",
    "                # 単語頻度辞書をカウントアップする\n",
    "                word_counts[key] += 1\n",
    "\n",
    "                # 抽出語情報をリストに追加する\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "            # 「未知語」\n",
    "            elif pos1 == \"未知語\":\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"未知語\"\n",
    "                key = (base, postag)\n",
    "\n",
    "                # 単語頻度辞書をカウントアップする\n",
    "                word_counts[key] += 1\n",
    "\n",
    "                # 抽出語情報をリストに追加する\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "        # 次の形態素へ\n",
    "        node = node.next\n",
    "\n",
    "# DataFrme 型に整える\n",
    "columns = [\n",
    "    \"文書ID\",\n",
    "    # \"単語ID\",\n",
    "    \"表層\",\n",
    "    \"品詞\",\n",
    "    \"カテゴリー\",\n",
    "    \"エリア\",\n",
    "    \"dict_key\",\n",
    "]\n",
    "docs_df = pd.DataFrame(words, columns=columns)\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(docs_df.shape)\n",
    "display(docs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acea1bc",
   "metadata": {},
   "source": [
    "抽出語の出現頻度をカウントする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e05e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「文書-抽出語」 表から単語の出現回数をカウントする\n",
    "word_list = []\n",
    "for i, (k, v) in enumerate(sorted(word_counts.items(), key=lambda x:x[1], reverse=True)):\n",
    "    word_list.append((i, k[0], v, k))\n",
    "\n",
    "# DataFrame 型に整える\n",
    "columns = [\n",
    "    \"単語ID\",\n",
    "    \"表層\",\n",
    "    \"出現頻度\",\n",
    "    \"dict_key\"\n",
    "]\n",
    "\n",
    "# DataFrame を表示する\n",
    "word_counts_df = pd.DataFrame(word_list, columns=columns)\n",
    "print(word_counts_df.shape)\n",
    "display(word_counts_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec185de7",
   "metadata": {},
   "source": [
    "### 1.1 カテゴリーやエリアごとのユーザーの注目ポイントの評価の違いを見つける"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f6c84",
   "metadata": {},
   "source": [
    "#### 1.1.1 「文書-抽出語」表の作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2d301",
   "metadata": {},
   "source": [
    "「文書-抽出語」表を作成する (出現回数 Top 1000語)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724abed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「単語出現回数」 表から出現回数Top 1000語のみ抽出する\n",
    "word_counts_1000_df = word_counts_df[0:1000]\n",
    "\n",
    "# 「文書-抽出語」 表も出現回数Top 150語のみに絞り込む\n",
    "merged_df = pd.merge(docs_df, word_counts_1000_df, how=\"inner\", on=\"dict_key\", suffixes=[\"\", \"_right\"])\n",
    "docs_1000_df = merged_df[[\"文書ID\", \"単語ID\", \"表層\", \"品詞\", \"カテゴリー\", \"エリア\", \"dict_key\"]]\n",
    "\n",
    "# 「カテゴリー,エリア」でクロス集計する\n",
    "cross_1000_df = pd.crosstab(\n",
    "    [\n",
    "        docs_1000_df['カテゴリー'], \n",
    "        docs_1000_df['エリア'], \n",
    "        docs_1000_df['文書ID']\n",
    "    ],\n",
    "    docs_1000_df['単語ID'], margins=False\n",
    ")\n",
    "cross_1000_df.columns = word_counts_1000_df[\"表層\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabc820",
   "metadata": {},
   "source": [
    "「文書-抽出語」表を {0,1} に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「文書-抽出語」 表を {0,1} に変換する\n",
    "cross_1000_df[cross_1000_df > 0] = 1\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(cross_1000_df.shape)\n",
    "display(cross_1000_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524d454",
   "metadata": {},
   "source": [
    "#### 1.1.2 共起行列を作成する (外部変数-抽出語)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「カテゴリー」のクロス集計と「エリア」のクロス集計を連結する\n",
    "aggregate_df = pd.concat(\n",
    "    [\n",
    "        cross_1000_df.groupby(level='カテゴリー').sum(),\n",
    "        cross_1000_df.groupby(level='エリア').sum()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(aggregate_df.shape)\n",
    "display(aggregate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384ac91",
   "metadata": {},
   "source": [
    "#### 1.1.3 Jaccard 係数を求める (外部変数-抽出語)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出語の出現回数を取得する\n",
    "word_counts = cross_1000_df.sum(axis=0).values\n",
    "\n",
    "# 属性(外部変数)出現数を取得する\n",
    "attr_counts = np.hstack(\n",
    "    [\n",
    "        all_df.value_counts('カテゴリー').values,\n",
    "        all_df.value_counts('エリア').values\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 共起行列の中身を Jaccard 係数に入れ替える\n",
    "jaccard_attrs_df = jaccard_attrs_coef(aggregate_df, attr_counts, word_counts, total=10000, conditional=False)\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(jaccard_attrs_df.shape)\n",
    "display(jaccard_attrs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc5e1b",
   "metadata": {},
   "source": [
    "#### 2.1.6 共起ネットワーク図"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b608a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "from scipy.sparse import csc_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# サブルーチン\n",
    "def sort_and_plot(name, group):\n",
    "\n",
    "    # 「カテゴリー」ごとに Jaccard 係数でソートする\n",
    "    sorted_columns = np.argsort(jaccard_attrs_df.loc[name].values)[::-1][:75]\n",
    "\n",
    "    # Jaccard 係数Top 75語をソートして抽出する\n",
    "    group_cross_df = group.iloc[:,sorted_columns]\n",
    "\n",
    "    # 共起行列を作成する\n",
    "    X = group_cross_df.values\n",
    "    X = csc_matrix(X)\n",
    "    Xc = (X.T * X)\n",
    "    Xc = np.triu(Xc.toarray())\n",
    "\n",
    "    # 共起行列を DataFrame に整える\n",
    "    group_cooccur_df = pd.DataFrame(Xc, columns=group_cross_df.columns, index=group_cross_df.columns)\n",
    "\n",
    "    # 共起行列の中身を Jaccard 係数に入れ替える\n",
    "    group_jaccard_df = jaccard_coef(group_cooccur_df, group_cross_df)\n",
    "\n",
    "    # 抽出語の出現回数を取得する\n",
    "    word_counts = group_cross_df.sum(axis=0).values\n",
    "\n",
    "    # プロットする\n",
    "    ax = fig.add_subplot(4, 3, i+1)\n",
    "    plot_cooccur_network_ax(ax, group_jaccard_df, word_counts, np.sort(group_jaccard_df.values.reshape(-1))[::-1][120])\n",
    "    ax.set_title(name)\n",
    "\n",
    "\n",
    "# プロットの準備\n",
    "fig = plt.figure(figsize=(20, 28))\n",
    "\n",
    "i = 0\n",
    "# カテゴリごとのループ\n",
    "for name, group in cross_1000_df.groupby(level='カテゴリー'):\n",
    "    # サブルーチンを呼ぶ\n",
    "    sort_and_plot(name, group)\n",
    "    i += 1\n",
    "\n",
    "    # エリアごとのループ\n",
    "    for sub_name, sub_group in group.groupby(level='エリア'):\n",
    "        # サブルーチンを呼ぶ\n",
    "        sort_and_plot(sub_name, sub_group)\n",
    "        i += 1\n",
    "\n",
    "# プロットの仕上げ\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95b8f1",
   "metadata": {},
   "source": [
    "### 1.2 高評価のエリアに倣って、低評価のエリアを改善するプランを提案する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2c7ba",
   "metadata": {},
   "source": [
    "#### 1.2.0 対照的な2エリアを選択する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コーディングルール\n",
    "coding_pos = [\"良い\",\"美味しい\",\"広い\",\"多い\",\"素晴らしい\",\"嬉しい\",\"気持ちよい\",\"楽しい\",\"近い\",\"大きい\",\"気持ち良い\",\"温かい\",\"早い\",\"優しい\",\"新しい\",\"暖かい\",\"快い\",\"明るい\",\"美しい\",\"可愛い\",\"満足\"]\n",
    "coding_neg = [\"古い\",\"無い\",\"高い\",\"悪い\",\"小さい\",\"狭い\",\"少ない\",\"寒い\",\"遅い\",\"熱い\",\"欲しい\",\"暑い\",\"冷たい\",\"遠い\",\"臭い\",\"暗い\",\"うるさい\",\"ない\",\"無い\",\"残念\",\"改善\",\"不満\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decabd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame を初期化する\n",
    "cross_1000_ps_df = cross_1000_df.copy()\n",
    "cross_1000_ps_df['ポジ'] = 0\n",
    "cross_1000_ps_df['ネガ'] = 0\n",
    "cross_1000_ps_df['総合1-2'] = 0\n",
    "cross_1000_ps_df['総合4-5'] = 0\n",
    "\n",
    "# コーディングルールを適用する (ポジ・ネガ)\n",
    "pos_index = docs_df['表層'].str.contains(\"|\".join(coding_pos))\n",
    "neg_index = docs_df['表層'].str.contains(\"|\".join(coding_neg))\n",
    "cross_1000_ps_df.loc[cross_1000_ps_df.index.get_level_values('文書ID').isin(docs_df.loc[pos_index, '文書ID']), 'ポジ'] = 1\n",
    "cross_1000_ps_df.loc[cross_1000_ps_df.index.get_level_values('文書ID').isin(docs_df.loc[neg_index, '文書ID']), 'ネガ'] = 1\n",
    "\n",
    "# コーディングルールを適用する (総合評価)\n",
    "cross_1000_ps_df.loc[cross_1000_ps_df.index.get_level_values('文書ID').isin(all_df[all_df['総合'] <=2].index), '総合1-2'] = 1\n",
    "cross_1000_ps_df.loc[cross_1000_ps_df.index.get_level_values('文書ID').isin(all_df[all_df['総合'] >=4].index), '総合4-5'] = 1\n",
    "cross_1000_ps_df = cross_1000_ps_df[['ポジ','ネガ','総合1-2','総合4-5']]\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(cross_1000_ps_df.shape)\n",
    "display(cross_1000_ps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0851666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「カテゴリー」のクロス集計と「エリア」のクロス集計を連結する\n",
    "aggregate_ps_df = pd.concat(\n",
    "    [\n",
    "        cross_1000_ps_df.groupby(level='カテゴリー').sum(),\n",
    "        cross_1000_ps_df.groupby(level='エリア').sum()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(aggregate_ps_df.shape)\n",
    "display(aggregate_ps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6926eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "import mca\n",
    "\n",
    "# ライブラリ mca による対応分析\n",
    "ncols = aggregate_ps_df.shape[1]\n",
    "mca_ben = mca.MCA(aggregate_ps_df, ncols=ncols, benzecri=False)\n",
    "\n",
    "# 行方向および列方向の値を取り出す\n",
    "row_coord = mca_ben.fs_r(N=2)\n",
    "col_coord = mca_ben.fs_c(N=2)\n",
    "\n",
    "# 固有値を求める\n",
    "eigenvalues = mca_ben.L\n",
    "total = np.sum(eigenvalues)\n",
    "# 寄与率を求める\n",
    "explained_inertia = 100 * eigenvalues / total\n",
    "\n",
    "# 行方向および列方向のラベルを取得する\n",
    "row_labels = aggregate_ps_df.index\n",
    "col_labels = aggregate_ps_df.columns\n",
    "\n",
    "# プロットする\n",
    "plot_coresp(row_coord, col_coord, row_labels, col_labels, explained_inertia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914307a",
   "metadata": {},
   "source": [
    "#### 1.2.1 ポジティブ意見の「文書-抽出語」表を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "from scipy.sparse import csc_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# コーディングルール\n",
    "coding_or = coding_pos\n",
    "\n",
    "# サブルーチン\n",
    "def sort_and_plot(name, group):\n",
    "\n",
    "    # 「カテゴリー」ごとに Jaccard 係数でソートする\n",
    "    sorted_columns = np.argsort(jaccard_attrs_df.loc[name].values)[::-1][:75]\n",
    "\n",
    "    # Jaccard 係数Top 75語をソートして抽出する\n",
    "    group_cross_df = group.iloc[:,sorted_columns]\n",
    "\n",
    "    # 共起行列を作成する\n",
    "    X = group_cross_df.values\n",
    "    X = csc_matrix(X)\n",
    "    Xc = (X.T * X)\n",
    "    Xc = np.triu(Xc.toarray())\n",
    "\n",
    "    # コーディングルールで絞り込む\n",
    "    index = docs_df['表層'].str.contains(\"|\".join(coding_or))\n",
    "    group_cross_df = group_cross_df[group_cross_df.index.get_level_values('文書ID').isin(docs_df.loc[index, '文書ID'])]\n",
    "\n",
    "    # 共起行列を DataFrame に整える\n",
    "    group_cooccur_df = pd.DataFrame(Xc, columns=group_cross_df.columns, index=group_cross_df.columns)\n",
    "\n",
    "    # 共起行列の中身を Jaccard 係数に入れ替える\n",
    "    group_jaccard_df = jaccard_coef(group_cooccur_df, group_cross_df)\n",
    "\n",
    "    # 抽出語の出現回数を取得する\n",
    "    word_counts = group_cross_df.sum(axis=0).values\n",
    "\n",
    "    # プロットする\n",
    "    ax = fig.add_subplot(4, 3, i+1)\n",
    "    plot_cooccur_network_with_code_ax(ax, group_jaccard_df, word_counts, np.sort(group_jaccard_df.values.reshape(-1))[::-1][120], coding_or)\n",
    "    ax.set_title(name)\n",
    "\n",
    "\n",
    "# プロットの準備\n",
    "fig = plt.figure(figsize=(20, 28))\n",
    "\n",
    "i = 0\n",
    "# カテゴリごとのループ\n",
    "for name, group in cross_1000_df.groupby(level='カテゴリー'):\n",
    "    # サブルーチンを呼ぶ\n",
    "    sort_and_plot(name, group)\n",
    "    i += 1\n",
    "\n",
    "    # エリアごとのループ\n",
    "    for sub_name, sub_group in group.groupby(level='エリア'):\n",
    "        # サブルーチンを呼ぶ\n",
    "        sort_and_plot(sub_name, sub_group)\n",
    "        i += 1\n",
    "\n",
    "# プロットの仕上げ\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf62cfa",
   "metadata": {},
   "source": [
    "#### 1.2.2 ネガティブ意見の「文書-抽出語」表を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5184d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "from scipy.sparse import csc_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# コーディングルール\n",
    "coding_or = coding_neg\n",
    "\n",
    "# サブルーチン\n",
    "def sort_and_plot(name, group):\n",
    "\n",
    "    # 「カテゴリー」ごとに Jaccard 係数でソートする\n",
    "    sorted_columns = np.argsort(jaccard_attrs_df.loc[name].values)[::-1][:75]\n",
    "\n",
    "    # Jaccard 係数Top 75語をソートして抽出する\n",
    "    group_cross_df = group.iloc[:,sorted_columns]\n",
    "\n",
    "    # 共起行列を作成する\n",
    "    X = group_cross_df.values\n",
    "    X = csc_matrix(X)\n",
    "    Xc = (X.T * X)\n",
    "    Xc = np.triu(Xc.toarray())\n",
    "\n",
    "    # コーディングルールで絞り込む\n",
    "    index = docs_df['表層'].str.contains(\"|\".join(coding_or))\n",
    "    group_cross_df = group_cross_df[group_cross_df.index.get_level_values('文書ID').isin(docs_df.loc[index, '文書ID'])]\n",
    "\n",
    "    # 共起行列を DataFrame に整える\n",
    "    group_cooccur_df = pd.DataFrame(Xc, columns=group_cross_df.columns, index=group_cross_df.columns)\n",
    "\n",
    "    # 共起行列の中身を Jaccard 係数に入れ替える\n",
    "    group_jaccard_df = jaccard_coef(group_cooccur_df, group_cross_df)\n",
    "\n",
    "    # 抽出語の出現回数を取得する\n",
    "    word_counts = group_cross_df.sum(axis=0).values\n",
    "\n",
    "    # プロットする\n",
    "    ax = fig.add_subplot(4, 3, i+1)\n",
    "    plot_cooccur_network_with_code_ax(ax, group_jaccard_df, word_counts, np.sort(group_jaccard_df.values.reshape(-1))[::-1][120], coding_or)\n",
    "    ax.set_title(name)\n",
    "\n",
    "\n",
    "# プロットの準備\n",
    "fig = plt.figure(figsize=(20, 28))\n",
    "\n",
    "i = 0\n",
    "# カテゴリごとのループ\n",
    "for name, group in cross_1000_df.groupby(level='カテゴリー'):\n",
    "    # サブルーチンを呼ぶ\n",
    "    sort_and_plot(name, group)\n",
    "    i += 1\n",
    "\n",
    "    # エリアごとのループ\n",
    "    for sub_name, sub_group in group.groupby(level='エリア'):\n",
    "        # サブルーチンを呼ぶ\n",
    "        sort_and_plot(sub_name, sub_group)\n",
    "        i += 1\n",
    "\n",
    "# プロットの仕上げ\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8b67f",
   "metadata": {},
   "source": [
    "#### 1.2.3 本文の参照 (カテゴリーごと)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01558796",
   "metadata": {},
   "source": [
    "「登別」と「道後」で「すばらしい」という単語が含まれている口コミを表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08190c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索条件\n",
    "search_index = \\\n",
    "    all_df['エリア'].isin(['01_登別', '05_道後']) & \\\n",
    "    (all_df['コメント'].str.contains('素晴らしい') | all_df['コメント'].str.contains('すばらしい'))\n",
    "\n",
    "# 検索する\n",
    "result_df = all_df[search_index]\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(result_df.shape)\n",
    "display(result_df.head())\n",
    "\n",
    "# CSV に保存する\n",
    "result_df.to_csv(\"output-1.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcbfdb",
   "metadata": {},
   "source": [
    "「東京」と「福岡」で「うるさい」という単語が含まれている口コミを表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a5acf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 検索条件\n",
    "search_index = \\\n",
    "    all_df['エリア'].isin(['08_東京', '10_福岡']) & \\\n",
    "    all_df['コメント'].str.contains('うるさい')\n",
    "\n",
    "# 検索する\n",
    "result_df = all_df[search_index]\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(result_df.shape)\n",
    "display(result_df.head())\n",
    "\n",
    "# CSV に保存する\n",
    "result_df.to_csv(\"output-2.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67e21e-d43b-402a-9dd6-aa5a28f436d1",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51996331-9be0-475a-9496-fcc4a88580f4",
   "metadata": {},
   "source": [
    "### A.1 辞書追加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df777b-13cf-474c-903d-b49ede039caa",
   "metadata": {},
   "source": [
    "#### A.1.1 辞書追加前の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3286e64-ebdf-4457-b2fb-9a338d828354",
   "metadata": {},
   "source": [
    "(1) 辞書追加前に MeCab の解析結果を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f6656-0f22-4a4d-af52-f1aff0556f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger(\"-r ../tools/usr/local/etc/mecabrc\")\n",
    "print(tagger.parse(\"この泉質は極上です\"))\n",
    "print(tagger.parse(\"この海鮮丼は美味しいです\"))\n",
    "print(tagger.parse(\"近くにスカイツリーがあります\"))\n",
    "print(tagger.parse(\"浴室にバスタオルがありません\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c776d73-84bd-44cf-a1ed-a9b824cf7d0f",
   "metadata": {},
   "source": [
    "(2) 辞書追加前に CaboCha の解析結果を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f58c2-59fe-494e-99f2-3694eea47fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "\n",
    "cp = CaboCha.Parser(\"-r ../tools/usr/local/etc/cabocharc\")\n",
    "print(cp.parse(\"この泉質は極上です\").toString(CaboCha.FORMAT_LATTICE))\n",
    "print(cp.parse(\"この海鮮丼は美味しいです\").toString(CaboCha.FORMAT_LATTICE))\n",
    "print(cp.parse(\"近くにスカイツリーがあります\").toString(CaboCha.FORMAT_LATTICE))\n",
    "print(cp.parse(\"浴室にバスタオルがありません\").toString(CaboCha.FORMAT_LATTICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6ff0a-8772-4a04-9cf5-7102f8e66bbd",
   "metadata": {},
   "source": [
    "#### A.1.2 辞書追加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee141c9-3d75-46be-a03f-96cf91a09ce6",
   "metadata": {},
   "source": [
    "(1) 追加したい形態素の情報を CSV ファイル(user_dic.csv)に追記する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434c851-29cc-48a4-81b2-33ecc779e4e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo '\"泉質\",-1,-1,1,名詞,一般,*,*,*,*,泉質,センシツ,センシツ,USER\"' >> ../tools/usr/local/lib/mecab/dic/ipadic/user_dic.csv\n",
    "!echo '\"海鮮丼\",-1,-1,1,名詞,一般,*,*,*,*,海鮮丼,カイセンドン,カイセンドン,USER\"' >> ../tools/usr/local/lib/mecab/dic/ipadic/user_dic.csv\n",
    "!echo '\"スカイツリー\",-1,-1,1,名詞,一般,*,*,*,*,スカイツリー,スカイツリー,スカイツリー,USER\"' >> ../tools/usr/local/lib/mecab/dic/ipadic/user_dic.csv\n",
    "!echo '\"バスタオル\",-1,-1,1,名詞,一般,*,*,*,*,バスタオル,バスタオル,バスタオル,USER\"' >> ../tools/usr/local/lib/mecab/dic/ipadic/user_dic.csv\n",
    "!cat ../tools/usr/local/lib/mecab/dic/ipadic/user_dic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c2b62-5e8e-4131-867e-076517607441",
   "metadata": {},
   "source": [
    "(2) CSVファイル(user_dic.csv)をコンパイルして辞書(user.dic)を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606743a0-5ee1-4e69-9277-537a8a2c32a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!../tools/usr/local/libexec/mecab/mecab-dict-index \\\n",
    "-d ../tools/usr/local/lib/mecab/dic/ipadic \\\n",
    "-u ../tools/usr/local/lib/mecab/dic/ipadic/user.dic \\\n",
    "-f utf-8 -t utf-8 \\\n",
    "../tools/usr/local/lib/mecab/dic/ipadic/user_dic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ddb3be-1784-42a6-accf-b5b793676dca",
   "metadata": {},
   "source": [
    "#### A.1.3 辞書追加後の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643bcbf-6ae0-46bb-8a0f-4f5eba1ac74e",
   "metadata": {},
   "source": [
    "(1) 辞書追加後に MeCab の解析結果を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91884c97-9f2e-463f-983c-a5c5ee954aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger(\"-r ../tools/usr/local/etc/mecabrc\")\n",
    "print(tagger.parse(\"この泉質は極上です\"))\n",
    "print(tagger.parse(\"この海鮮丼は美味しいです\"))\n",
    "print(tagger.parse(\"近くにスカイツリーがあります\"))\n",
    "print(tagger.parse(\"浴室にバスタオルがありません\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c298dd-c2b3-489b-a172-9e0356a71eda",
   "metadata": {
    "tags": []
   },
   "source": [
    "(2) 辞書追加後に CaboCha の解析結果を確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dea6e3-2cf2-40b3-8184-45519787d614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "\n",
    "cp = CaboCha.Parser(\"-r ../tools/usr/local/etc/cabocharc\")\n",
    "print(cp.parse(\"この泉質は極上です\").toString(CaboCha.FORMAT_LATTICE))\n",
    "print(cp.parse(\"この海鮮丼は美味しいです\").toString(CaboCha.FORMAT_LATTICE))\n",
    "print(cp.parse(\"近くにスカイツリーがあります\").toString(CaboCha.FORMAT_LATTICE))\n",
    "print(cp.parse(\"浴室にバスタオルがありません\").toString(CaboCha.FORMAT_LATTICE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gssm2023:Python",
   "language": "python",
   "name": "conda-env-gssm2023-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
